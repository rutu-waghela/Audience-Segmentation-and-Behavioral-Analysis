# -*- coding: utf-8 -*-
"""CLV & Churn preditction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oTFc7w7VvL4yIWMLbiU356DAeJiGpcUN
"""

# Preprocessing and feature engineering
import pandas as pd

import numpy as np
from datetime import datetime

retail_data = pd.read_excel('/content/sample_data/retail_data.xlsx')
# Convert 'Purchase Date' to datetime format
retail_data['Purchase Date'] = pd.to_datetime(retail_data['Purchase Date'])

# Aggregate data for each customer to compute total purchases and CLV
customer_data = retail_data.groupby('Customer ID').agg({
    'Total Purchase Amount': 'sum',  # CLV
    'Purchase Date': ['min', 'max', 'count'],  # First and last purchase, purchase count
    'Age': 'first',  # Age
    'Gender': 'first',  # Gender
    'Churn': 'first'  # Churn
}).reset_index()

# Flatten MultiIndex columns
customer_data.columns = ['Customer ID', 'CLV', 'First Purchase', 'Last Purchase', 'Purchase Count', 'Age', 'Gender', 'Churn']

# Calculate Recency (days since last purchase)
reference_date = retail_data['Purchase Date'].max()
customer_data['Recency'] = (reference_date - customer_data['Last Purchase']).dt.days

# Calculate Frequency (average number of purchases per year)
customer_data['Frequency'] = customer_data['Purchase Count'] / (
    (customer_data['Last Purchase'] - customer_data['First Purchase']).dt.days / 365.25 + 1)

# Handle any missing or inconsistent values (if applicable)
customer_data['Gender'] = customer_data['Gender'].fillna('Unknown')

# Drop rows with NaN values (if any)
customer_data = customer_data.dropna()

# Preview the processed data
display(customer_data.head())

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Prepare data for CLV prediction
X_clv = customer_data[['Age', 'Recency', 'Frequency', 'Purchase Count']]
y_clv = customer_data['CLV']

# Split the data into training and testing sets
X_train_clv, X_test_clv, y_train_clv, y_test_clv = train_test_split(X_clv, y_clv, test_size=0.2, random_state=42)

# Train a Random Forest Regressor
rf_clv = RandomForestRegressor(random_state=42, n_estimators=100)
rf_clv.fit(X_train_clv, y_train_clv)

# Make predictions
y_pred_clv = rf_clv.predict(X_test_clv)

# Evaluate the model
mse_clv = mean_squared_error(y_test_clv, y_pred_clv)
r2_clv = r2_score(y_test_clv, y_pred_clv)

mse_clv, r2_clv

# Ensure Customer IDs are included in the dataset
X_test_clv['Customer ID'] = customer_data.loc[X_test_clv.index, 'Customer ID']

# Generate predictions for the testing set
y_pred_clv = rf_clv.predict(X_test_clv.drop(columns=['Customer ID']))

# Combine Customer IDs, actual, and predicted CLV values
results = pd.DataFrame({
    'Customer ID': X_test_clv['Customer ID'],
    'Actual CLV': y_test_clv.values,
    'Predicted CLV': y_pred_clv
})

# Display the first few rows of results for verification
print(results.head())

# Save the results for further analysis
results.to_csv('predicted_clv_per_customer.csv', index=False)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Define features (X) and target (y)
X_churn = customer_data[['Age', 'Recency', 'Frequency', 'Purchase Count']]
y_churn = customer_data['Churn']

# Split the data into training and testing sets
X_train_churn, X_test_churn, y_train_churn, y_test_churn = train_test_split(
    X_churn, y_churn, test_size=0.2, random_state=42
)

# Train a Random Forest Classifier
rf_churn = RandomForestClassifier(random_state=42, n_estimators=100)
rf_churn.fit(X_train_churn, y_train_churn)

# Predict churn on the test set
y_pred_churn = rf_churn.predict(X_test_churn)

# Evaluate the model's accuracy
accuracy = accuracy_score(y_test_churn, y_pred_churn)
print(f"Accuracy: {accuracy}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_test_churn, y_pred_churn))

# Confusion matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test_churn, y_pred_churn))

# Check feature importance
feature_importances = pd.DataFrame({
    'Feature': X_churn.columns,
    'Importance': rf_churn.feature_importances_
}).sort_values(by='Importance', ascending=False)

print("\nFeature Importances:")
print(feature_importances)

# Combine actual and predicted churn status
churn_results = pd.DataFrame({
    'Customer ID': X_test_churn.index,  # Replace with the appropriate Customer ID column
    'Actual Churn': y_test_churn.values,
    'Predicted Churn': y_pred_churn
})

print("\nSample Results:")
print(churn_results.head())

# Save results for further analysis
churn_results.to_csv('churn_prediction_results.csv', index=False)